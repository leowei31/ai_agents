# AI Agent Papers

- [A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond](https://arxiv.org/abs/2508.11957): AI agents should integrate perception, abstract reasoning, hierarchical planning, and flexible communication while ensuring safety, interpretability and adhering to ethical standards. Challenges are robustness under domain shift, explainability of behaviors, and value alignment. Planning: RL/graph based planning for task abstraction. Chain of thought for multi-step reasoning. RL for interactions with environment and learning policy. Reflexion for learning via trial and error through verbal self-reflection. Chain of Hindsight uses past outputs to improve model performance. Memory: Short term/textual (RAG, recent interactions) and long term/parametric (fine-tuning, prompt tunning). Tools: Search engines, code interpreters, robotic arms, etc. The biggest challenges when it comes to tools is hallucination, input errors and planning complexity. Perception modules: These essentially are the eyes/ears for the agent, they convert sensory data into structure representations. Representation and Abstraction Layers: Self supervised or contrastive learning approaches. Planning and Reasoning Engines: Hiearchical RL structures decisionmaking at multiple levels of abstraction. Interaction and Communication Interfaces: Natural language interface, multi agent communication protocols. Challenges and limitations: Explainability, generalization/transfer of knowledge across domains. Opportunities: Continual learning, symbolic reasoning models, governance and coordiation of multi agent systems. 

- [AI Agents That Matter](https://arxiv.org/abs/2407.01502): Cost controlled AI agent evals. Better agent design with optimizing for both accuracy and cost. Downstream tasks have distinct benchmarking needs. Agent shouldn't overfit to benchmarks. Agent evals lack reproducibility. Propose to separate benchmarks intended for model innovation from those targeting application specific performance. 

- [The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey](https://arxiv.org/abs/2404.11584): Single Agent (one LLM handles reasoning, planning and tool use) don't benefit from feedback from other agents. Multi Agent (multiple LLM with different personas and tool access, vercal-a lead agent directs others in a hiearchical style, horizontal-agents operate equally with shared communication channels). Reasoning enables flexible adaptation to new information. Planning includes task decomposition, multi-plan selection, reflection, memory. Tool calling is key for fetching external data and executing actions. Single Agent Examples: ReAct(Reason + Act)-Alternates between thoughts and actions, this reduce hallucination compared to chain-of-thought. RAISE-Builds on ReAct by adding short and long term memory. Reflexion-Uses self reflection. AutoGPT+P-Integrate object detection and classical planning with LLM planning for robotics. LATS(language agent tree search)-Monte Carlo style tree search with LLM reflection. Multi Agent Examples: Embodied LLM Agents in Organized Teams-Vertical + Horizontal, having a human to lead accelerates task complextion by 10%. Teams with leaders focus on more information exchange. DyLAN(Dynamic LLM-Agent Network)-Horizontal structure, selects agents based on past performance. Easily adapts and improves reasoning and code generation. AgentVerse-Staged model. Works best with horizontal collaboration. MetaGPT-Address chatter by enforcing structured outputs, use a publish-subscribe model to control message flow and minimize noise. 

- [FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251): Human evaluators are costly, long form texts from LLMs tend to mix accurate and inaccurate information. FACTSCORE breaks down texts into atomic facts and computes the percentages these facts are confirmed by a reliable fact source. GPT-4 and ChatGPT outperform public models in factual precision. FACTScore = (# of supported facts) / (# of total atomic facts). Atomic facts like statements with a verifiable piece of information like Person A was born in Year 1999. These are usually annotated with human. But for scalability, the paper used instructGPT. 